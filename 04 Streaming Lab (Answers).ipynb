{"cells":[{"cell_type":"markdown","source":["-sandbox\n<div style=\"text-align: center; line-height: 0; padding-top: 9px;\">\n  <img src=\"https://cdn2.hubspot.net/hubfs/438089/docs/training/dblearning-banner.png\" alt=\"Databricks Learning\" width=\"555\" height=\"64\">\n</div>"],"metadata":{}},{"cell_type":"markdown","source":["-sandbox\n&copy; 2018 Databricks, Inc. All rights reserved.<br/>"],"metadata":{}},{"cell_type":"markdown","source":["### Challenges\n* Larger Data\n* Faster data and decisions - seconds, minutes, hours not days or weeks after it is created\n* Streaming Pipelines can be hard\n* Realtime Dashboards and alerts - for the holiday season, promotional campaigns, track falling or rising trends\n\n### Azure Databricks Solutions\n* Deploy Event Hubs with a click of button\n* Connect Azure Databricks with a click of a button\n* Easy streaming pipelines almost the same as batch - SQL, Python, Scala, Java & R\n* Make this data avialable on Storage or ADL to end users in minutes not days or weeks. \n\n### Why Initech Needs Streaming\n* Sales up or down (rolling 24 hours, 1 hour), to identify trends that are good or bad\n* Holidays and promotions - how are the performing in real time"],"metadata":{}},{"cell_type":"markdown","source":["-sandbox\n## What is Structured Streaming?\n\n<div style=\"width: 100%\">\n  <div style=\"margin: auto; width: 800px\">\n    <img src=\"http://spark.apache.org/docs/latest/img/structured-streaming-stream-as-a-table.png\"/>\n  </div>\n</div>\n\nData is appended to the Input Table every _trigger interval_. For instance, if the trigger interval is 1 second, then new data is appended to the Input Table every seconds. (The trigger interval is analogous to the _batch interval_ in the legacy RDD-based Streaming API.)"],"metadata":{}},{"cell_type":"markdown","source":["####Azure Databricks for Streaming Analytics, Alerts, ETL & Data Engineers \n\n![arch](https://kpistoropen.blob.core.windows.net/collateral/roadshow/azure_roadshow_sde.png)"],"metadata":{}},{"cell_type":"markdown","source":["##![Spark Logo Tiny](https://kpistoropen.blob.core.windows.net/collateral/roadshow/logo_spark_tiny.png) *Part-1:* Create Streaming DataFrame"],"metadata":{}},{"cell_type":"code","source":["#schema for our streaming DataFrame\n\nfrom pyspark.sql.types import *\nschema = StructType([ \\\n  StructField(\"orderUUID\", StringType(), True), \\\n  StructField(\"productId\", IntegerType(), True), \\\n  StructField(\"userId\", IntegerType(), True), \\\n  StructField(\"quantity\", IntegerType(), True), \\\n  StructField(\"discount\", DoubleType(), True), \\\n  StructField(\"orderTimestamp\", TimestampType(), True)])"],"metadata":{},"outputs":[],"execution_count":7},{"cell_type":"code","source":["%fs ls /mnt/training-sources/initech/streaming/orders/data/"],"metadata":{},"outputs":[],"execution_count":8},{"cell_type":"code","source":["#streaming DataFrame reader for data on Azure Storage\n\nstreaming_df = spark.readStream \\\n    .schema(schema) \\\n    .option(\"maxFilesPerTrigger\", 1) \\\n    .csv(\"dbfs:/mnt/training-sources/initech/streaming/orders/data/part-*\")"],"metadata":{},"outputs":[],"execution_count":9},{"cell_type":"markdown","source":["## What can we do with this Streaming DataFrame?\n\nIf you run the following cell, you'll get a continuously updating display of the number of records read from the stream so far. Note that we're just calling `display()` on our DataFrame, _exactly_ as if it were a DataFrame reading from a static data source.\n\nTo stop the continuous update, just cancel the query."],"metadata":{}},{"cell_type":"code","source":["display(streaming_df)"],"metadata":{},"outputs":[],"execution_count":11},{"cell_type":"markdown","source":["##![Spark Logo Tiny](https://kpistoropen.blob.core.windows.net/collateral/roadshow/logo_spark_tiny.png) *Part-2:* Transform Streaming DataFrame"],"metadata":{}},{"cell_type":"markdown","source":["### It's just a DataFrame\n\nWe can use normal DataFrame transformations on our streaming DataFrame. For example, let's group the number of orders by productId\n\n<img src=\"https://spark.apache.org/docs/latest/img/structured-streaming-example-model.png\"/>"],"metadata":{}},{"cell_type":"code","source":["from pyspark.sql.functions import *\ntop_products = streaming_df.groupBy(\"productId\").agg(sum(col(\"quantity\")).alias(\"total_units_by_product\")).orderBy(desc(\"total_units_by_product\"))"],"metadata":{},"outputs":[],"execution_count":14},{"cell_type":"markdown","source":["* Call `display` on `top_products`\n* Turn the streaming table into a streaming bar chart"],"metadata":{}},{"cell_type":"code","source":["display(top_products)"],"metadata":{},"outputs":[],"execution_count":16},{"cell_type":"markdown","source":["##![Spark Logo Tiny](https://kpistoropen.blob.core.windows.net/collateral/roadshow/logo_spark_tiny.png) *Part-3:* Streaming Joins"],"metadata":{}},{"cell_type":"markdown","source":["### Streaming Joins\n\nGrouping by unkown product IDs is not that that exciting. Let's join the stream with the product lookup data set\n* Use the join key productId\n* Hint: Since both DataFrames have the same column name `productId`\n* Use the duplicated columns trick documented here: https://docs.azuredatabricks.net/spark/latest/faq/join-two-dataframes-duplicated-column.html"],"metadata":{}},{"cell_type":"markdown","source":["Load the product lookup data from Azure Storage"],"metadata":{}},{"cell_type":"code","source":["product_lookup = spark.read.parquet(\"/mnt/training-sources/initech/productsFull/\")"],"metadata":{},"outputs":[],"execution_count":20},{"cell_type":"markdown","source":["Join the `streaming_df` with `product_lookup` on `productId`\n* Hint: https://docs.azuredatabricks.net/spark/latest/faq/join-two-dataframes-duplicated-column.html"],"metadata":{}},{"cell_type":"code","source":["#TO-DO\njoined_df = streaming_df.join(product_lookup, \"ProductID\")"],"metadata":{},"outputs":[],"execution_count":22},{"cell_type":"code","source":["display(joined_df)"],"metadata":{},"outputs":[],"execution_count":23},{"cell_type":"markdown","source":["##![Spark Logo Tiny](https://kpistoropen.blob.core.windows.net/collateral/roadshow/logo_spark_tiny.png) *Part-4:* Calculate a Streaming Dashboard - Revenue by Product Name"],"metadata":{}},{"cell_type":"markdown","source":["### Calculate the Total Revenue by Product Name\n\n* Now that we have the product `Name` let's use that instead of the `productId` to `groupBy`\n* Also let's calculate the total revenue instead of just units sold\n  * Use the `quanity` column and the `StandardCost` column"],"metadata":{}},{"cell_type":"code","source":["#TO-DO\ntop_products = joined_df.groupBy(\"Name\").agg(sum(col(\"quantity\")*col(\"StandardCost\")).alias(\"total_revenue_by_product\")).orderBy(desc(\"total_revenue_by_product\"))"],"metadata":{},"outputs":[],"execution_count":26},{"cell_type":"code","source":["display(top_products)"],"metadata":{},"outputs":[],"execution_count":27},{"cell_type":"code","source":[""],"metadata":{},"outputs":[],"execution_count":28}],"metadata":{"name":"04 Streaming Lab (Answers)","notebookId":667211845502864},"nbformat":4,"nbformat_minor":0}
