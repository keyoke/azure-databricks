{"cells":[{"cell_type":"markdown","source":["-sandbox\n<div style=\"text-align: center; line-height: 0; padding-top: 9px;\">\n  <img src=\"https://cdn2.hubspot.net/hubfs/438089/docs/training/dblearning-banner.png\" alt=\"Databricks Learning\" width=\"555\" height=\"64\">\n</div>"],"metadata":{}},{"cell_type":"markdown","source":["-sandbox\n&copy; 2018 Databricks, Inc. All rights reserved.<br/>"],"metadata":{}},{"cell_type":"markdown","source":["### Challenges\n* Business wants better product recommendations and conversion on website and emails\n* Data Science spends most of their time connecting and wrangling data, very little on actual data science\n* Data Science is hard to scale from sample data to large data sets\n\n\n### Azure Databricks Solutions\n* With all the data in one place (Azure Storage, Azure Data Lake), Easy for DS to spend time on DS\n* Azure Databricks Scales to ML on GB, TB, PB of Data\n* Easily go into production with ML (save results to CosmosDB)\n\n### Why Initech uses Azure Databricks for ML\n* Millions of users and 100,000s of prodcuts, product reccomendations need more than a single machine\n* Easy APIs for newer data science team\n* Store results in CosmosDB for online serving (emails, website, etc)"],"metadata":{}},{"cell_type":"markdown","source":["####Azure Databricks for Machine Learning and Data Scientists\n![arch](https://kpistoropen.blob.core.windows.net/collateral/roadshow/azure_roadshow_ml.png)"],"metadata":{}},{"cell_type":"markdown","source":["# ![Spark Logo Tiny](https://s3-us-west-2.amazonaws.com/curriculum-release/images/105/logo_spark_tiny.png) Providing Product Recommendations\n\nOne of the most common uses of big data is to predict what users want.  This allows Google to show you relevant ads, Amazon to recommend relevant products, and Netflix to recommend movies that you might like.  This lab will demonstrate how we can use Apache Spark to recommend products to a user.  \n\nWe will start with some basic techniques, and then use the SparkML library's Alternating Least Squares method to make more sophisticated predictions. Here are the SparkML [Python docs](https://spark.apache.org/docs/latest/api/python/pyspark.ml.html) and the [Scala docs](https://spark.apache.org/docs/latest/api/scala/#org.apache.spark.ml.package).\n\nFor this lesson, we will use around 900,000 historical product ratings from our company Initech.\n\nIn this lab:\n* *Part 0*: Exploratory Analysis\n* *Part 1*: Collaborative Filtering\n* *Part 2*: Analysis"],"metadata":{}},{"cell_type":"markdown","source":["## ![Spark Logo Tiny](https://s3-us-west-2.amazonaws.com/curriculum-release/wiki-book/general/logo_spark_tiny.png) *Part 0:* Exploratory Analysis\n\nLet's start by taking a look at our data.  It's already mounted in `/mnt/training-msft/ratings.parquet` table for us.  Exploratory analysis should answer questions such as:\n\n* How many observations do I have?\n* What are the features?\n* Do I have missing values?\n* What do summary statistics (e.g. mean and variance) tell me about my data?\n\nStart by importing the data.  Bind it to `productRatings` by running the cell below"],"metadata":{}},{"cell_type":"code","source":["product_ratings = spark.read.parquet(\"dbfs:/mnt/training-sources/initech/productRatings/\")"],"metadata":{},"outputs":[],"execution_count":7},{"cell_type":"code","source":["display(product_ratings)"],"metadata":{},"outputs":[],"execution_count":8},{"cell_type":"markdown","source":["Take a count of the data using the `count()` DataFrame method."],"metadata":{}},{"cell_type":"code","source":["#TO-DO\nproduct_ratings.count()"],"metadata":{},"outputs":[],"execution_count":10},{"cell_type":"markdown","source":["#### Let's look at what these product_ids mean?\n\n* There is a product lookup dataset in parquet located here: `dbfs:/mnt/training-sources/initech/productsShort/`"],"metadata":{}},{"cell_type":"code","source":["#TO-DO\nproduct_df = spark.read.parquet(\"dbfs:/mnt/training-sources/initech/productsShort/\") #use the Spark parquet reader like the productRatings DataFrame above to read the products data"],"metadata":{},"outputs":[],"execution_count":12},{"cell_type":"code","source":["display(product_df)"],"metadata":{},"outputs":[],"execution_count":13},{"cell_type":"markdown","source":["## ![Spark Logo Tiny](https://s3-us-west-2.amazonaws.com/curriculum-release/wiki-book/general/logo_spark_tiny.png) *Part 1:* Collaborative Filtering\n\nThe image below (from [Wikipedia][collab]) shows an example of predicting of the user's rating using collaborative filtering. At first, people rate different items (like videos, products, articles, images, games). After that, the system is making predictions about a user's rating for an item, which the user has not rated yet. These predictions are built upon the existing ratings of other users, who have similar ratings with the active user. For instance, in the image below the system has made a prediction, that the active user will not like the video.  \n![collaborative filtering](https://courses.edx.org/c4x/BerkeleyX/CS100.1x/asset/Collaborative_filtering.gif)\n\n[SparkML]: http://spark.apache.org/docs/latest/ml-guide.html\n[collab]: https://en.wikipedia.org/?title=Collaborative_filtering\n[collab2]: http://recommender-systems.org/collaborative-filtering/"],"metadata":{}},{"cell_type":"code","source":[" #We'll hold out 60% for training, 20% of our data for validation, and leave 20% for testing \nseed = 1800009193\n(training_df, validation_df, test_df) = product_ratings.randomSplit([.6, .2, .2], seed=seed)"],"metadata":{},"outputs":[],"execution_count":15},{"cell_type":"code","source":["display(training_df)"],"metadata":{},"outputs":[],"execution_count":16},{"cell_type":"markdown","source":["### My Ratings\n* Fill in your ratings for the above `product_df`\n* Pick 5-10 product ids to rate\n* Choose your ratings be 1-5"],"metadata":{}},{"cell_type":"code","source":["#TO-DO\nmy_user_id = 0\nmy_rated_products = [\n     (1, my_user_id, 5), # Replace with your ratings.\n     (2, my_user_id, 5),\n     (3, my_user_id, 5),\n     (4, my_user_id, 5),\n     (6, my_user_id, 1),\n     (7, my_user_id, 1),\n     (9, my_user_id, 1),\n     (9, my_user_id, 1),\n     (9, my_user_id, 1),\n     ]"],"metadata":{},"outputs":[],"execution_count":18},{"cell_type":"code","source":["my_ratings_df = spark.createDataFrame(my_rated_products, ['product_id','user_id','rating'])"],"metadata":{},"outputs":[],"execution_count":19},{"cell_type":"markdown","source":["Join your ratings with the `product_df` to see your ratings with the product metadata"],"metadata":{}},{"cell_type":"code","source":["display(my_ratings_df.join(product_df, ['product_id']))"],"metadata":{},"outputs":[],"execution_count":21},{"cell_type":"markdown","source":["Union your ratings with the `trainingDF` to see your ratings with the product metadata"],"metadata":{}},{"cell_type":"code","source":["training_with_my_ratings_DF = training_df.union(my_ratings_df)"],"metadata":{},"outputs":[],"execution_count":23},{"cell_type":"markdown","source":["### Alternating Least Squares\n\nIn this part, we will use the Apache Spark ML Pipeline implementation of Alternating Least Squares, [ALS (Python)](http://spark.apache.org/docs/latest/api/python/pyspark.ml.html#pyspark.ml.recommendation.ALS) or [ALS (Scala)](https://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.ml.recommendation.ALS). ALS takes a training dataset (DataFrame) and several parameters that control the model creation process.\n\nThe process we will use for determining the best model is as follows:\n1. Pick a set of model parameters. The most important parameter to model is the *rank*, which is the number of columns in the Users matrix (green in the diagram above) or the number of rows in the Products matrix (blue in the diagram above). In general, a lower rank will mean higher error on the training dataset, but a high rank may lead to [overfitting](https://en.wikipedia.org/wiki/Overfitting).  We will train models with a rank of 2 using the `trainingDF` dataset.\n\n2. Set the appropriate parameters on the `ALS` object:\n    * The \"User\" column will be set to the values in our `user_id` DataFrame column.\n    * The \"Item\" column will be set to the values in our `product_id` DataFrame column.\n    * The \"Rating\" column will be set to the values in our `rating` DataFrame column.\n    * We'll be using a regularization parameter of 0.1.\n    \n   **Note**: Read the documentation for the ALS class **carefully**. It will help you accomplish this step.\n3. Have the ALS output transformation (i.e., the result of `ALS.fit()`) produce a _new_ column\n   called \"prediction\" that contains the predicted value.\n\n4. Create multiple models using `ALS.fit()`, one for each of our rank values. We'll fit \n   against the training data set (`trainingDF`).\n\n5. We'll run our prediction against our validation data set (`validationDF`) and check the error.\n\n6. Use `.setColdStartStrategy(\"drop\")` so that the model can deal with missing values."],"metadata":{}},{"cell_type":"code","source":["from pyspark.ml.recommendation import ALS\n\n# Let's initialize our ALS learner\nals = ALS()\n\n# Now we set the parameters for the method\n(als.setPredictionCol(\"prediction\")\n   .setUserCol(\"user_id\")\n   .setItemCol(\"product_id\")\n   .setRatingCol(\"rating\")\n   .setMaxIter(5)\n   .setSeed(seed)\n   .setRegParam(0.1)\n   .setRank(2)\n   .setColdStartStrategy(\"drop\")\n)"],"metadata":{},"outputs":[],"execution_count":25},{"cell_type":"markdown","source":["### Validation: \nLet's see how we did against know ratings"],"metadata":{}},{"cell_type":"code","source":["#TO-DO\nmodel = als.fit(training_with_my_ratings_DF) #fill in with training_with_my_ratings_DF\n# Run the model to create a prediction. Predict against the validationDF.\npredict_df = model.transform(validation_df)"],"metadata":{},"outputs":[],"execution_count":27},{"cell_type":"code","source":["display(predict_df)"],"metadata":{},"outputs":[],"execution_count":28},{"cell_type":"markdown","source":["## ![Spark Logo Tiny](https://s3-us-west-2.amazonaws.com/curriculum-release/wiki-book/general/logo_spark_tiny.png) *Part 2:* Your Recommendations:\nLet's look at what ALS recommended for your user based on your ratings"],"metadata":{}},{"cell_type":"code","source":["#TO-DO\n#Filter the predictions DF for your user id something like \"user_id = ID\"\npredictions = model.recommendForAllUsers(10)\nmy_predictions = predictions.filter(\"user_id = 0\")"],"metadata":{},"outputs":[],"execution_count":30},{"cell_type":"code","source":["display(my_predictions)"],"metadata":{},"outputs":[],"execution_count":31},{"cell_type":"code","source":["from pyspark.sql.functions import *\nmy_recs = my_predictions.select(\"user_id\", explode(\"recommendations\").alias(\"recommendations\")).select(\"user_id\", \"recommendations.product_id\", \"recommendations.rating\")"],"metadata":{},"outputs":[],"execution_count":32},{"cell_type":"code","source":["display(my_recs)"],"metadata":{},"outputs":[],"execution_count":33},{"cell_type":"markdown","source":["###Join"],"metadata":{}},{"cell_type":"code","source":["from pyspark.sql.functions import *\nmy_recs = my_predictions.select(\"user_id\", explode(\"recommendations\").alias(\"recommendations\")).select(\"user_id\", \"recommendations.product_id\", \"recommendations.rating\").join(product_df, ['product_id'])"],"metadata":{},"outputs":[],"execution_count":35},{"cell_type":"code","source":["display(my_recs)"],"metadata":{},"outputs":[],"execution_count":36}],"metadata":{"name":"06 ML Lab (Answers)","notebookId":667211845502953},"nbformat":4,"nbformat_minor":0}
